{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Gridsearch in Surprise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s get started implementing the  GridSearchCV class from Surprise so we can find the best parameters for our SVD recommendation engine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let’s import the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# import needed surprise libraries\n",
    "from surprise import Reader, Dataset\n",
    "from surprise.model_selection import GridSearchCV\n",
    "from surprise.prediction_algorithms import SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s bring in our dataset - for demonstration purposes, this tim we'll use the built-in movielens data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use movielens-100K\n",
    "data = Dataset.load_builtin('ml-100k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're using your own dataset, please uncomment the code below and specify your rating scale and column names in line three to read in those values as a Surprise dataset. The columns must be representive of (1) the items, (2) the ratings, (3) the associated users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# read in values as a Surprise dataset\n",
    "reader = Reader(rating_scale=(1,5), line_format=('item rating user'))\n",
    "data = Dataset.load_from_df(df[['item_id', 'rating_id', 'user_id']],reader)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⏰ NOTE: This process can be especially time-consuming depending on the amount of film ratings data you read in. With the movielens data, the GridSearch below took twenty minutes on my machine.\n",
    "\n",
    "Through trial and error and feedback from my instructor at Flatiron, I developed a process that can help others bring immediate structure to their search and perhaps newfound insight into the data being worked with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9434885112563943\n",
      "{'lr_all': 0.0075, 'n_factors': 75, 'reg_all': 0.025, 'biased': True}\n",
      "<surprise.prediction_algorithms.matrix_factorization.SVD object at 0x7fd549a36f28>\n"
     ]
    }
   ],
   "source": [
    "# tune hyperparameters using GridSearch to get improved model\n",
    "\n",
    "param_grid = {'lr_all': [0.0025,0.005,0.0075], 'n_factors': [75, 100, 150],\n",
    "             'reg_all': [0.015,0.02,0.025], 'biased': [True, False]}\n",
    "gs = GridSearchCV(SVD, param_grid, measures=['rmse'], cv=3)\n",
    "\n",
    "gs.fit(data)\n",
    "\n",
    "# best RMSE score\n",
    "print(gs.best_score['rmse'])\n",
    "\n",
    "# combination of parameters that gave the best RMSE score\n",
    "print(gs.best_params['rmse'])\n",
    "\n",
    "# best accuracy results for the chosen measure, averaged over all splits\n",
    "print(gs.best_estimator['rmse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "RMSE: 0.9434885112563943\n",
    "\n",
    "Winners: 'lr_all': 0.0075, 'n_factors': 75, 'reg_all': 0.025, 'biased': True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dissecting the Parameter Grid**\n",
    "Above, we used the default values according to four significant GridsearchCV parameters and created a range around them. This means that when we identify our winner, we can further tighten that range around the winning parameter this time to continue increasing accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Default SVD](../images/default-svd-hyperparameters.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding our Results**\n",
    "After running our model, we saw our winning parameters. Here’s what this tells us and what our next steps could be:\n",
    "\n",
    "* ‘lr_all’: 0.0075 was the winner and shows an increase from our default\n",
    "* ‘n_factors’: 75 was the winner and shows a decrease from the default\n",
    "* ‘reg_all’: 0.025 was the winner and shows an increase from our default\n",
    "* ‘biased’: True was the winner, this is the only default value that won in our param_grid\n",
    "\n",
    "Because our default metrics did not win across the board, we likely have an improvement on our model!\n",
    "\n",
    "Due to the fact that this was run on a split of our data (remember our cv) and not the full set, even if it didn’t quite measure up to our default accuracy - this could likely mean an increase in accuracy. Now, with this dataset, you can continue trying to tweak the gridsearch within ranges centering those winning parameters - or if you’re strapped for time, you can move forward and specify these parameters when running your SVD model to compare accuracies!\n",
    "\n",
    "If any questions arise during this process, please feel free to reach out via stanton.sierraerin@gmail.com"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
